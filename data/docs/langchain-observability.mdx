---
date: 2025-08-19
id: langchain-observability-with-opentelemetry
tags: [SigNoz Cloud, Self-Host]
title: LangChain/LangGraph Observability with SigNoz
description: Enable observability and monitoring in your LangChain and LangGraph application with OpenTelemetry to send traces, logs, and metrics to SigNoz
---

## Overview

This guide walks you through enabling observability and monitoring for your Python-based [LangChain](https://www.langchain.com/) application and streaming telemetry data to SigNoz Cloud using OpenTelemetry. By the end of this setup, you'll be able to monitor AI-specific operations such as agent reasoning steps, tool invocations, API calls, and intermediate chain executions within LangChain, with detailed spans capturing request durations, tool inputs and outputs, model responses, and metadata throughout the agentâ€™s decision-making process.

Instrumenting your agent workflows with telemetry enables full observability across the reasoning and action pipeline. This is especially valuable when building production-grade AI applications, where insight into agent behavior, latency bottlenecks, tool call performance, and response accuracy is essential. With SigNoz, you can trace each user request end-to-endâ€”from the initial prompt through every intermediate reasoning step, tool execution, and final answerâ€”and continuously improve performance, reliability, and user experience.

To get started, check out our example LangChain trip planner agent with OpenTelemetry-based observability/monitoring (via OpenInference). View the full repository [here](https://github.com/SigNoz/langchain-monitoring-demo).

You can also check out our LangChain SigNoz MCP agent [here](https://github.com/SigNoz/signoz-mcp-demo).

## Prerequisites

- A Python application using **Python 3.8+**
- LangChain/LangGraph integrated into your app
- Basic understanding of AI Agents and tool calling workflow
- SigNoz setup (choose one):
  - [SigNoz Cloud account](https://signoz.io/teams/) with an active ingestion key
  - Self-hosted SigNoz instance
- `pip` installed for managing Python packages
- Internet access to send telemetry data to SigNoz Cloud
- _(Optional but recommended)_ A Python virtual environment to isolate dependencies

## Instrument your LangChain Python application

To capture detailed telemetry from LangChain/LangGraph without modifying your core application logic, we use [OpenInference](https://arize.com/docs/ax/learn/tracing-concepts/what-is-openinference), a community-driven standard that provides pre-built instrumentation for popular AI frameworks like LangChain, built on top of OpenTelemetry. This allows you to trace your LangChain application with minimal configuration.

Check out detailed instructions on how to set up OpenInference instrumentation in your LangChain application over [here](https://pypi.org/project/openinference-instrumentation-langchain/).

<Tabs>
<TabItem value="No Code" label="No Code(Recommended)" default>

No-code auto-instrumentation is recommended for quick setup with minimal code changes. It's ideal when you want to get observability up and running without modifying your application code and are leveraging standard instrumentor libraries.

**Step 1:** Install the necessary packages in your Python environment.

```bash
pip install \
  opentelemetry-distro \
  opentelemetry-exporter-otlp \
  opentelemetry-instrumentation-httpx \
  opentelemetry-instrumentation-system-metrics \
  langgraph \
  langchain \
  openinference-instrumentation-langchain
```

**Step 2:** Add Automatic Instrumentation

```bash
opentelemetry-bootstrap --action=install
```

**Step 3:** Configure logging level

To ensure logs are properly captured and exported, configure the root logger to emit logs at the INFO level or higher:

```python
import logging

logging.getLogger().setLevel(logging.INFO)
```

This sets the minimum log level for the root logger to INFO, which ensures that `logger.info()` calls and higher severity logs (WARNING, ERROR, CRITICAL) are captured by the OpenTelemetry logging auto-instrumentation and sent to SigNoz.

**Step 4:** Run an example

```python
from langchain.agents import create_agent

def add_numbers(a: int, b: int) -> int:
    """Add two numbers together and return the result."""
    return a + b

agent = create_agent(
    model="openai:gpt-5-mini",
    tools=[add_numbers],
    system_prompt="You are a helpful math tutor who can do calculations using the provided tools.",
)

# Run the agent
agent.invoke(
    {"messages": [{"role": "user", "content": "what is 42 + 58?"}]},
)
```

> ðŸ“Œ Note: Ensure that the `OPENAI_API_KEY` environment variable is properly defined with your API key before running the code.

**Step 5:** Run your application with auto-instrumentation

```bash
OTEL_RESOURCE_ATTRIBUTES="service.name=<service_name>" \
OTEL_EXPORTER_OTLP_ENDPOINT="https://ingest.<region>.signoz.cloud:443" \
OTEL_EXPORTER_OTLP_HEADERS="signoz-ingestion-key=<your_ingestion_key>" \
OTEL_EXPORTER_OTLP_PROTOCOL=grpc \
OTEL_TRACES_EXPORTER=otlp \
OTEL_METRICS_EXPORTER=otlp \
OTEL_LOGS_EXPORTER=otlp \
OTEL_PYTHON_LOG_CORRELATION=true \
OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED=true \
opentelemetry-instrument <your_run_command>
```

- **`<service_name>`**Â is the name of your service
- Set the `<region>` to match your SigNoz Cloud [region](https://signoz.io/docs/ingestion/signoz-cloud/overview/#endpoint)
- Replace `<your_ingestion_key>` with your SigNoz [ingestion key](https://signoz.io/docs/ingestion/signoz-cloud/keys/)
- Replace `<your_run_command>` with the actual command you would use to run your application. For example: `python main.py`

<Admonition type="info">
  Using self-hosted SigNoz? Most steps are identical. To adapt this guide, update the endpoint and
  remove the ingestion key header as shown in
  <a href="/docs/ingestion/cloud-vs-self-hosted#cloud-to-self-hosted">Cloud â†’ Self-Hosted</a>.
</Admonition>

</TabItem>

<TabItem value="Code" label="Code" default>

Code-based instrumentation gives you fine-grained control over your telemetry configuration. Use this approach when you need to customize resource attributes, sampling strategies, or integrate with existing observability infrastructure.

**Step 1:** Install OpenInference and OpenTelemetry related packages

```bash
pip install openinference-instrumentation-langchain \
opentelemetry-exporter-otlp \
opentelemetry-sdk \
langgraph \
langchain
```

**Step 2:** Import the necessary modules in your Python application

```python
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import Resource
from openinference.instrumentation.langchain import LangChainInstrumentor
```

**Step 3:** Set up the OpenTelemetry Tracer Provider to send traces directly to SigNoz Cloud

```python
resource = Resource.create({"service.name": "<service_name>"})
provider = TracerProvider(resource=resource)
span_exporter = OTLPSpanExporter(
    endpoint="https://ingest.<region>.signoz.cloud:443/v1/traces",
    headers={"signoz-ingestion-key": "<your-ingestion-key>"},
)
provider.add_span_processor(BatchSpanProcessor(span_exporter))
```

- **`<service_name>`**Â is the name of your service
- Set theÂ **`<region>`**Â to match your SigNoz CloudÂ [**region**](https://signoz.io/docs/ingestion/signoz-cloud/overview/#endpoint)
- ReplaceÂ **`<your-ingestion-key>`**Â with your SigNozÂ [**ingestion key**](https://signoz.io/docs/ingestion/signoz-cloud/keys/)

<Admonition type="info">
  Using self-hosted SigNoz? Most steps are identical. To adapt this guide, update the endpoint and
  remove the ingestion key header as shown in
  <a href="/docs/ingestion/cloud-vs-self-hosted#cloud-to-self-hosted">Cloud â†’ Self-Hosted</a>.
</Admonition>

**Step 4:** Instrument LangChain using OpenInference

Use the `LangChainInstrumentor` from OpenInference to automatically trace LangChain operations with your OpenTelemetry setup:

```python
LangChainInstrumentor().instrument()
```

> ðŸ“Œ Important: Place this code at the start of your application logic â€” before any LangChain/LangGraph functions are called or used â€” to ensure telemetry is correctly captured.

**Step 5:** Run an example

```python
from langchain.agents import create_agent

def add_numbers(a: int, b: int) -> int:
    """Add two numbers together and return the result."""
    return a + b

agent = create_agent(
    model="openai:gpt-5-mini",
    tools=[add_numbers],
    system_prompt="You are a helpful math tutor who can do calculations using the provided tools.",
)

# Run the agent
agent.invoke(
    {"messages": [{"role": "user", "content": "what is 42 + 58?"}]},
)
```

> ðŸ“Œ Note: Ensure that the `OPENAI_API_KEY` environment variable is properly defined with your API key before running the code.

</TabItem>
</Tabs>

Your LangChain/LangGraph commands should now automatically emit traces, spans, and attributes.

Finally, you should be able to view this data in Signoz Cloud under the traces tab:

<figure data-zoomable align="center">
  <img src="/img/docs/llm/langchain/langchain-trace.webp" alt="Traces View" />
  <figcaption>
    <i>Traces of your LangChain Application</i>
  </figcaption>
</figure>

When you click on a trace ID in SigNoz, you'll see a detailed view of the trace, including all associated spans, along with their events and attributes:

<figure data-zoomable align="center">
  <img src="/img/docs/llm/langchain/langchain-detailed-trace.webp" alt="Detailed Traces View" />
  <figcaption>
    <i>Detailed traces view of your LangChain Application</i>
  </figcaption>
</figure>

## Instrumenting LangChain Applications in JavaScript

You can instrument your LangChain/LangGraph applications in JavaScript using the [OpenInference LangChain Instrumentor](https://www.npmjs.com/package/@arizeai/openinference-instrumentation-langchain) package.

For detailed guidance on instrumenting JavaScript applications with OpenTelemetry and connecting them to SigNoz, see the [SigNoz OpenTelemetry JavaScript instrumentation docs](https://signoz.io/docs/instrumentation/opentelemetry-javascript/).
