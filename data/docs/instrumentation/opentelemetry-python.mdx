---
date: 2025-12-16
id: python

title: Python OpenTelemetry Instrumentation
description: Send traces to SigNoz using OpenTelemetry instrumentation for Python frameworks like Django, Flask, FastAPI, and Celery.
doc_type: howto
---

This guide shows you how to instrument your Python application with OpenTelemetry and send traces to SigNoz. The auto-instrumentation approach works with Django, Flask, FastAPI, Falcon, Celery, and most Python libraries out of the box.

<KeyPointCallout title="Using self-hosted SigNoz?" defaultCollapsed={true}>
Most steps are identical. To adapt this guide, update the endpoint and remove the ingestion key header as shown in [Cloud → Self-Hosted](https://signoz.io/docs/ingestion/cloud-vs-self-hosted/#cloud-to-self-hosted).
</KeyPointCallout>

## Prerequisites

- Python 3.8 or newer
- A SigNoz Cloud account or self-hosted SigNoz instance
- Your application code

Tested with Python 3.11 and OpenTelemetry Python SDK v1.27.0.

## Send traces to SigNoz

<Tabs entityName="deployment">
<TabItem value="vm" label="VM" default>

<KeyPointCallout title="What classifies as VM?" defaultCollapsed={true}>
A VM is a virtual computer that runs on physical hardware. This includes:
- **Cloud VMs**: AWS EC2, Google Compute Engine, Azure VMs, DigitalOcean Droplets
- **On-premise VMs**: VMware, VirtualBox, Hyper-V, KVM
- **Bare metal servers**: Physical servers running Linux/Unix directly

Use this section if you're deploying your Python application directly on a server or VM without containerization.
</KeyPointCallout>

### Step 1. Install OpenTelemetry packages

```bash
pip install opentelemetry-distro opentelemetry-exporter-otlp
```

### Step 2. Install instrumentation for your dependencies

This command detects your installed packages and adds the corresponding instrumentation libraries:

```bash
opentelemetry-bootstrap --action=install
```

<Admonition type="info">
Run this after installing all your application dependencies. It will only instrument packages that are already installed.
</Admonition>

### Step 3. Set environment variables

```bash
export OTEL_RESOURCE_ATTRIBUTES="service.name=<service-name>"
export OTEL_EXPORTER_OTLP_ENDPOINT="https://ingest.<region>.signoz.cloud:443"
export OTEL_EXPORTER_OTLP_HEADERS="signoz-ingestion-key=<your-ingestion-key>"
export OTEL_EXPORTER_OTLP_PROTOCOL="grpc"
```

Replace the following:
- `<region>`: Your SigNoz Cloud region (`us`, `eu`, or `in`). See [endpoints](https://signoz.io/docs/ingestion/signoz-cloud/overview/#endpoint).
- `<your-ingestion-key>`: Your SigNoz [ingestion key](https://signoz.io/docs/ingestion/signoz-cloud/keys/).
- `<service-name>`: A descriptive name for your service (e.g., `payment-service`).

### Step 4. Run your application

```bash
opentelemetry-instrument <your_run_command>
```

See [Framework instrumentation](#framework-instrumentation) below for framework-specific commands.

</TabItem>
<TabItem value="k8s" label="Kubernetes">

For Python applications on Kubernetes, you can use:
- **K8s OTel Operator** (recommended) — Auto-injects instrumentation without code changes
- **OTel Collector Agent** — Manual instrumentation with collector handling forwarding

<Tabs entityName="k8s-method">
<TabItem value="k8s-otel-operator" label="K8s OTel Operator" default>

The <a href="https://opentelemetry.io/docs/kubernetes/operator" target="_blank" rel="noopener noreferrer nofollow">OpenTelemetry Operator</a> manages collectors and auto-instrumentation in Kubernetes.

### Step 1. Install cert-manager

```bash
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.16.1/cert-manager.yaml
kubectl wait --for=condition=Available deployments/cert-manager -n cert-manager
```

### Step 2. Install OpenTelemetry Operator

```bash
kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/download/v0.116.0/opentelemetry-operator.yaml
```

### Step 3. Create the OpenTelemetry Collector instance

Create `otel-collector.yaml`:

```yaml:otel-collector.yaml
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: otel-collector
spec:
  mode: deployment
  config: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
    processors:
      batch: {}
    exporters:
      otlp:
        endpoint: "ingest.<region>.signoz.cloud:443"
        tls:
          insecure: false
        headers:
          "signoz-ingestion-key": "<your-ingestion-key>"
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch]
          exporters: [otlp]
```

Replace `<region>` and `<your-ingestion-key>` with your SigNoz Cloud values.

```bash
kubectl apply -f otel-collector.yaml
```

### Step 4. Create the Instrumentation instance

Create `instrumentation.yaml`:

```yaml:instrumentation.yaml
apiVersion: opentelemetry.io/v1alpha1
kind: Instrumentation
metadata:
  name: python-instrumentation
spec:
  exporter:
    endpoint: http://otel-collector-collector:4318
  propagators:
    - tracecontext
    - baggage
  python:
    image: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-python:latest
```

```bash
kubectl apply -f instrumentation.yaml
```

### Step 5. Add annotations to your deployment

```yaml:deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: python-app
spec:
  selector:
    matchLabels:
      app: python-app
  template:
    metadata:
      labels:
        app: python-app
      annotations:
        instrumentation.opentelemetry.io/inject-python: "true"
        instrumentation.opentelemetry.io/otel-python-platform: "glibc"  # or "musl" for Alpine
        resource.opentelemetry.io/service.name: "python-app"
    spec:
      containers:
      - name: app
        image: python-app:latest
        ports:
        - containerPort: 8080
```

```bash
kubectl apply -f deployment.yaml
```

</TabItem>
<TabItem value="otel-collector-agent" label="OTel Collector Agent">

Install the OTel Collector agent in your cluster first. See [Kubernetes infrastructure metrics](https://signoz.io/docs/tutorial/kubernetes-infra-metrics/). The collector handles forwarding to SigNoz, so your app doesn't need the ingestion key.

### Step 1. Add OpenTelemetry to requirements.txt

```txt
opentelemetry-distro
opentelemetry-exporter-otlp
```

### Step 2. Update your Dockerfile

```dockerfile:Dockerfile
RUN pip install --no-cache-dir -r requirements.txt
RUN opentelemetry-bootstrap --action=install

ENV OTEL_RESOURCE_ATTRIBUTES=service.name=<service-name>
ENV OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
ENV OTEL_EXPORTER_OTLP_PROTOCOL=grpc

# See Framework instrumentation section for the correct CMD
CMD ["opentelemetry-instrument", "python", "app.py"]
```

</TabItem>
</Tabs>

</TabItem>
<TabItem value="windows" label="Windows">

### Step 1. Install OpenTelemetry packages

```bash
pip install opentelemetry-distro opentelemetry-exporter-otlp
opentelemetry-bootstrap --action=install
```

### Step 2. Set environment variables

```powershell
$env:OTEL_RESOURCE_ATTRIBUTES = "service.name=<service-name>"
$env:OTEL_EXPORTER_OTLP_ENDPOINT = "https://ingest.<region>.signoz.cloud:443"
$env:OTEL_EXPORTER_OTLP_HEADERS = "signoz-ingestion-key=<your-ingestion-key>"
$env:OTEL_EXPORTER_OTLP_PROTOCOL = "grpc"
```

### Step 3. Run your application

```bash
opentelemetry-instrument <your_run_command>
```

See [Framework instrumentation](#framework-instrumentation) below for framework-specific commands.

</TabItem>
<TabItem value="docker" label="Docker">

### Step 1. Update your Dockerfile

```dockerfile
RUN pip install opentelemetry-distro opentelemetry-exporter-otlp
RUN opentelemetry-bootstrap --action=install

ENV OTEL_RESOURCE_ATTRIBUTES=service.name=<service-name>
ENV OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.<region>.signoz.cloud:443
ENV OTEL_EXPORTER_OTLP_HEADERS=signoz-ingestion-key=<your-ingestion-key>
ENV OTEL_EXPORTER_OTLP_PROTOCOL=grpc

# See Framework instrumentation section for the correct CMD
CMD ["opentelemetry-instrument", "python", "app.py"]
```

Replace `<region>`, `<your-ingestion-key>`, and `<service-name>` with your values.

### Step 2. Build and run

```bash
docker build -t <image-name> .
docker run -d -p <host-port>:<container-port> <image-name>
```

</TabItem>
</Tabs>

## Framework instrumentation

Choose your framework below to see the specific run command. The setup steps above are the same for all frameworks.

<LibraryTabs
    className="mb-8"
    defaultCategory="all"
    defaultLibrary="django"
    categoryLabels={{
        web: 'Web Frameworks',
        worker: 'Background Workers',
    }}
>
    <LibraryTab value="django" label="Django" category="web">

        ### Prerequisites

        Set the `DJANGO_SETTINGS_MODULE` environment variable:

        ```bash
        export DJANGO_SETTINGS_MODULE=myproject.settings
        ```

        ### Run command

        ```bash
        opentelemetry-instrument python manage.py runserver --noreload
        ```

        <Admonition type="warning">
        Always use `--noreload` with Django. The auto-reload mechanism spawns child processes that break OpenTelemetry instrumentation.
        </Admonition>

        <KeyPointCallout title="For Docker users" defaultCollapsed={true}>
        ```dockerfile
        CMD ["opentelemetry-instrument", "python", "manage.py", "runserver", "0.0.0.0:8000", "--noreload"]
        ```
        </KeyPointCallout>

    </LibraryTab>
    <LibraryTab value="flask" label="Flask" category="web">

        ### Run command

        ```bash
        opentelemetry-instrument flask run --no-reload
        ```

        Or if running directly:

        ```bash
        opentelemetry-instrument python app.py
        ```

        <Admonition type="warning">
        Always use `--no-reload` with Flask. The reloader spawns a child process that breaks OpenTelemetry instrumentation. Also avoid `FLASK_ENV=development` as it enables the reloader.
        </Admonition>

        <KeyPointCallout title="For Docker users" defaultCollapsed={true}>
        ```dockerfile
        CMD ["opentelemetry-instrument", "flask", "run", "--host=0.0.0.0", "--no-reload"]
        ```

        Or:

        ```dockerfile
        CMD ["opentelemetry-instrument", "python", "app.py"]
        ```
        </KeyPointCallout>

    </LibraryTab>
    <LibraryTab value="fastapi" label="FastAPI" category="web">

        ### Run command

        ```bash
        opentelemetry-instrument uvicorn main:app --host 0.0.0.0 --port 8000
        ```

        <Admonition type="warning">
        Do not use `--reload` with Uvicorn when instrumenting. The reload mode spawns new processes that break instrumentation.
        </Admonition>

        <Admonition type="info">
        Uvicorn's `--workers` flag is not supported with `opentelemetry-instrument`. Use Gunicorn with Uvicorn workers instead: `gunicorn -k uvicorn.workers.UvicornWorker main:app`
        </Admonition>

        <KeyPointCallout title="For Docker users" defaultCollapsed={true}>
        ```dockerfile
        CMD ["opentelemetry-instrument", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
        ```
        </KeyPointCallout>

    </LibraryTab>
    <LibraryTab value="falcon" label="Falcon" category="web">

        ### Run command (with Gunicorn)

        ```bash
        opentelemetry-instrument gunicorn app:api --bind 0.0.0.0:8000
        ```

        Or with Waitress:

        ```bash
        opentelemetry-instrument waitress-serve --port=8000 app:api
        ```

        <KeyPointCallout title="For Docker users" defaultCollapsed={true}>
        ```dockerfile
        CMD ["opentelemetry-instrument", "gunicorn", "app:api", "--bind", "0.0.0.0:8000"]
        ```
        </KeyPointCallout>

    </LibraryTab>
    <LibraryTab value="celery" label="Celery" category="worker">

        ### Run command

        ```bash
        opentelemetry-instrument celery -A tasks worker --loglevel=info
        ```

        Replace `tasks` with your Celery app module name.

        <KeyPointCallout title="For Docker users" defaultCollapsed={true}>
        ```dockerfile
        CMD ["opentelemetry-instrument", "celery", "-A", "tasks", "worker", "--loglevel=info"]
        ```
        </KeyPointCallout>

        <Admonition type="info">
        Celery instrumentation captures task execution spans, including task name, arguments, and status. Both the worker and the code that enqueues tasks should be instrumented for full trace propagation.
        </Admonition>

        <KeyPointCallout title="Celery with prefork workers (advanced)" defaultCollapsed={true}>
        Celery uses the prefork worker model by default. The OpenTelemetry SDK is not fork-safe, so you must initialize it in each worker process using the `worker_process_init` signal. Add this to your Celery app file:

        ```python
        from celery.signals import worker_process_init
        from opentelemetry.instrumentation.celery import CeleryInstrumentor
        from opentelemetry import trace
        from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
        from opentelemetry.sdk.resources import Resource
        from opentelemetry.sdk.trace import TracerProvider
        from opentelemetry.sdk.trace.export import BatchSpanProcessor

        @worker_process_init.connect(weak=False)
        def init_celery_tracing(*args, **kwargs):
            CeleryInstrumentor().instrument()
            resource = Resource.create({})
            trace.set_tracer_provider(TracerProvider(resource=resource))
            span_processor = BatchSpanProcessor(OTLPSpanExporter())
            trace.get_tracer_provider().add_span_processor(span_processor)
        ```

        This ensures each worker process creates its own tracer instance. Only required for workers, not for the code that enqueues tasks.
        </KeyPointCallout>

    </LibraryTab>
    <LibraryTab value="hypercorn-unicorn" label="Hypercorn/Unicorn" category="web">

        <Admonition type="warning">
        **Hypercorn** and **Unicorn** are **not supported** by OpenTelemetry auto-instrumentation due to fork-safety limitations.
        </Admonition>

        ### Why they don't work

        The OpenTelemetry SDK components (`BatchSpanProcessor`, `PeriodicExportingMetricReader`, `BatchLogProcessor`) spawn background threads and use locks, which are not fork-safe (see <a href="https://bugs.python.org/issue6721" target="_blank" rel="noopener noreferrer nofollow">Python issue #6721</a>).

        Most ASGI servers work around this using `register_at_fork` hooks to reinitialize after forking. However, Hypercorn and Unicorn use the `spawn` method to start worker processes, which doesn't invoke these hooks—making the workaround ineffective.

        ### Recommended alternative

        Use Gunicorn with Uvicorn workers instead:

        ```bash
        opentelemetry-instrument gunicorn -k uvicorn.workers.UvicornWorker main:app --bind 0.0.0.0:8000
        ```

        <KeyPointCallout title="For Docker users" defaultCollapsed={true}>
        ```dockerfile
        CMD ["opentelemetry-instrument", "gunicorn", "-k", "uvicorn.workers.UvicornWorker", "main:app", "--bind", "0.0.0.0:8000"]
        ```
        </KeyPointCallout>

        This gives you the same ASGI capabilities with proper OpenTelemetry support. See <a href="https://github.com/pgjones/hypercorn/issues/215" target="_blank" rel="noopener noreferrer nofollow">this issue</a> for updates on native Hypercorn support.

    </LibraryTab>
</LibraryTabs>

<details>
<ToggleHeading>

## Running with Gunicorn or uWSGI

</ToggleHeading>

**Gunicorn** works out of the box. No extra setup needed unless you use the `--preload` flag. If you do use `--preload`, see the <a href="https://github.com/open-telemetry/opentelemetry-python/tree/main/docs/examples/fork-process-model" target="_blank" rel="noopener noreferrer nofollow">post_fork hook example</a>.

**uWSGI** requires one of these options:
- Add `lazy-apps = true` to your uWSGI config (recommended, simplest fix)
- Or implement a post_fork hook (see <a href="https://opentelemetry-python.readthedocs.io/en/latest/examples/fork-process-model/README.html" target="_blank" rel="noopener noreferrer nofollow">example</a>)

<KeyPointCallout title="Why do some setups need extra config?" defaultCollapsed={true}>
OpenTelemetry's span exporter uses a background thread. When a server forks worker processes, this thread doesn't copy over correctly, causing spans to get stuck.

**Gunicorn** loads your app in each worker after forking (so the thread starts fresh). With `--preload`, it loads before forking, causing the same issue.

**uWSGI** loads your app before forking by default. Setting `lazy-apps = true` makes it load after forking instead.
</KeyPointCallout>

</details>

## Validate

With your application running, verify traces are being sent to SigNoz:

1. Trigger an action in your app that generates a web request. Hit the endpoint a few times.
2. In SigNoz, open the **Services** tab and click **Refresh**. Your application should appear.
3. Go to the **Traces** tab to see your application's traces.

<details>
<ToggleHeading>

## Troubleshooting

</ToggleHeading>

### Why don't traces appear in SigNoz?

**Check environment variables are set:**

```bash
echo $OTEL_EXPORTER_OTLP_ENDPOINT
echo $OTEL_RESOURCE_ATTRIBUTES
```

**Verify network connectivity:**

```bash
# For SigNoz Cloud
curl -v https://ingest.<region>.signoz.cloud:443/v1/traces
```

**Enable console exporter to verify spans are being created:**

```bash
OTEL_TRACES_EXPORTER=console opentelemetry-instrument <your_run_command>
```

If you see JSON span output in your terminal but traces don't appear in SigNoz, the issue is with export configuration (endpoint, auth, or network). If no output appears, the instrumentation isn't capturing your requests.

### Why do multi-worker servers (Uvicorn, Gunicorn) drop spans?

Application servers that spawn multiple worker processes require special handling because the OpenTelemetry SDK isn't fork-safe.

- **Uvicorn** with `--workers` flag is not supported. Use Gunicorn with Uvicorn workers instead:
  ```bash
  opentelemetry-instrument gunicorn -k uvicorn.workers.UvicornWorker main:app
  ```
- **Hypercorn/Unicorn** are not supported due to fork-safety issues. See the [Hypercorn/Unicorn tab](#framework-instrumentation) for details and workarounds.
- **Gunicorn with `--preload`** or **uWSGI** need extra config. See [Running with Gunicorn or uWSGI](#running-with-gunicorn-or-uwsgi).

### Hot reload breaks instrumentation

Don't run your app in reloader/hot-reload mode. For Flask, avoid `FLASK_ENV=development`. For Django, use `--noreload`. For Uvicorn/FastAPI, don't use `--reload`.

### gRPC installation issues

If `grpcio` installation fails, use the HTTP exporter instead:

```bash
pip install opentelemetry-exporter-otlp-proto-http
```

Then set `OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf`.

### Database calls not showing in traces

Auto-instrumentation detects and instruments common database libraries. Ensure you've run `opentelemetry-bootstrap --action=install` after installing your database drivers.

**PostgreSQL note**: `psycopg2` is preferred over `psycopg2-binary` for auto-instrumentation. If you must use `psycopg2-binary`, you may need to use manual instrumentation with `Psycopg2Instrumentor().instrument(skip_dep_check=True)`.

Common database instrumentations installed by bootstrap:
- PostgreSQL: `opentelemetry-instrumentation-psycopg2`
- MySQL: `opentelemetry-instrumentation-pymysql` or `opentelemetry-instrumentation-mysql`
- MongoDB: `opentelemetry-instrumentation-pymongo`
- Redis: `opentelemetry-instrumentation-redis`
- SQLAlchemy: `opentelemetry-instrumentation-sqlalchemy`

Check <a href="https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation" target="_blank" rel="noopener noreferrer nofollow">supported versions</a> for compatibility with your library versions.

</details>

<details>
<ToggleHeading>

## Setup OpenTelemetry Collector (Optional)

</ToggleHeading>

### What is the OpenTelemetry Collector?

Think of the OTel Collector as a middleman between your app and SigNoz. Instead of your application sending data directly to SigNoz, it sends everything to the Collector first, which then forwards it along.

### Why use it?

- **Cleaning up data** — Filter out noisy traces you don't care about, or remove sensitive info before it leaves your servers.
- **Keeping your app lightweight** — Let the Collector handle batching, retries, and compression instead of your application code.
- **Adding context automatically** — The Collector can tag your data with useful info like which Kubernetes pod or cloud region it came from.
- **Future flexibility** — Want to send data to multiple backends later? The Collector makes that easy without changing your app.

See [Switch from direct export to Collector](https://signoz.io/docs/opentelemetry-collection-agents/opentelemetry-collector/switch-to-collector/) for step-by-step instructions to convert your setup.

For more details, see [Why use the OpenTelemetry Collector?](https://signoz.io/docs/opentelemetry-collection-agents/opentelemetry-collector/why-to-use-collector/) and the [Collector configuration guide](https://signoz.io/docs/opentelemetry-collection-agents/opentelemetry-collector/configuration/).

</details>

## Next steps

- [Add manual instrumentation](https://signoz.io/docs/instrumentation/python/manual-instrumentation) for custom spans and attributes
- [Send logs from Python](https://signoz.io/docs/userguide/python-logs-auto-instrumentation/) using auto-instrumentation
- [Set up alerts](https://signoz.io/docs/alerts-management/notification-channel/slack/) for your Python application
- [Create dashboards](https://signoz.io/docs/userguide/manage-dashboards/) to visualize metrics

**Sample applications**:
- <a href="https://github.com/SigNoz/sample-django" target="_blank" rel="noopener noreferrer nofollow">Django sample app</a>
- <a href="https://github.com/SigNoz/sample-flask-app" target="_blank" rel="noopener noreferrer nofollow">Flask sample app</a>
- <a href="https://github.com/SigNoz/sample-fastAPI-app" target="_blank" rel="noopener noreferrer nofollow">FastAPI sample app</a>
